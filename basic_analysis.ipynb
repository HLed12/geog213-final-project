{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2a7b68a5-220b-4d72-aca5-983a3aa753b3",
   "metadata": {},
   "source": [
    "# OVERVIEW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89ef99ed-bb72-423b-a578-452c8be6f57a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fd7bfa0-4d24-4f78-af33-97acf889d3d8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a016592a-c291-482b-a400-e8148c1e64b4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd5c951c-9d15-46ae-816a-8bae625c0619",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8ddfc205-a6b6-409b-8782-eacb54706613",
   "metadata": {},
   "source": [
    "## Setting up Libraries, S3 client, and Dask Dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "227094c6-524f-495a-ac9c-39b58e2e0e12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import dask_geopandas as dg\n",
    "import utils as ut"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdf37d16-171b-41a5-bd98-5b4667d0f4fb",
   "metadata": {},
   "source": [
    "### ATTENTION:\n",
    "User must follow below instructions and replace the AWS access key and secret access key. This is crucial for the analysis to work.\n",
    "\n",
    "##### Utilizing data from: https://source.coop/repositories/wherobots/usa-structures/description\n",
    "\n",
    "All data on Source Cooperative, are hosted on AWS S3 bucket. In order to access them, you need credentials that you can generate on Source Cooperative website. Atfer logging in, click on your name at the top right corner, and then click on your username. Then navigate to \"Manage\" page on the left side. At the bottom of this page you will find a section called \"API Keys\". If no key has been generated before, generate a new one and then copy the values for each of the following keys, and paste them in the following cell.\n",
    "\n",
    "source.coop website: https://source.coop/\n",
    "\n",
    "###### Source: https://github.com/github.com/HamedAlemo/vector-data-tutorial/scalable_vector_analysis.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ab1349c-b5c6-4535-aaf6-b59305157d62",
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################\n",
    "#   Read Above 'ATTENTION' Note  #\n",
    "##################################\n",
    "\n",
    "AWS_ACCESS_KEY_ID = \"<YOUR ACCESS KEY>\"\n",
    "AWS_SECRET_ACCESS_KEY = \"<YOUR SECRET ACCESS KEY>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "499c7078-c992-4729-bcfd-a52375556369",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "s3_client = boto3.client('s3',\n",
    "                         aws_access_key_id = AWS_ACCESS_KEY_ID, \n",
    "                         aws_secret_access_key = AWS_SECRET_ACCESS_KEY,\n",
    "                         endpoint_url='https://data.source.coop'\n",
    "                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43a1a228-be07-4a9e-b90f-0edb2a26d4a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.distributed import Client, LocalCluster\n",
    "cluster = LocalCluster()\n",
    "client = Client(cluster)\n",
    "print(client.dashboard_link)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50f04db7-57b3-4b37-bbe2-2d6dbd207160",
   "metadata": {},
   "source": [
    "##### Local path for downloading the data\n",
    "\n",
    "If running the analysis file within the 'saved'//mounted folder and do NOT wish to save the raw data to your computer - delete the\n",
    "first local_path and uncomment (remove initial #) on the second one. NOTE: you will have to re-install each time you load this image up if you do that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a64648eb-97b5-4b0c-a2b1-6eac13db9b0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "local_path = \"./data/\"    # saves data to machine\n",
    "# local_path = '/home/gisuser/data/'   # deletes data after closing the container"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e99d25b6-245d-4247-ad0e-a77a6fdc6b92",
   "metadata": {},
   "source": [
    "## US Structures Data (Installing all of it)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d023db5f-909e-404c-8800-06e4fcaa328c",
   "metadata": {},
   "source": [
    "Change the blocksize value if your computer is stronger. This currently sets each block to 16 Megabytes. More details can be found here (ctrl + f 'blocksize'): https://coderzcolumn.com/tutorials/python/dask-dataframes-guide-to-work-with-large-tabular-datasets\n",
    "\n",
    "Install time will vary. As of 12/9/2024, the dataset is 10 geoparquets each equating to ~2 Gigabytes. This took ~40 minutes to install on my personal machine: ~ 3 Yr Old Laptop, Windows 11, 16 Gb RAM (Docker is limited to 12 Gb), Intel(R) Core(TM) i7-10750H CPU @ 2.60GHz, 2592 Mhz, 6 Core(s), 12 Logical Processor(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96dfbcc5-c128-4d37-9ff5-75d24774af1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "structure_ddf = ut.get_US_structures_all(s3_client, local_path, blocksize = \"16M\") # \"256M\" is regular block size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "665433e6-630c-4151-a96d-a05c37a4fd76",
   "metadata": {},
   "outputs": [],
   "source": [
    "structure_ddf.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ce25adc-a727-423d-9f80-d7d0d2648230",
   "metadata": {},
   "outputs": [],
   "source": [
    "structure_ddf.tail(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0df8b3c-087e-4439-996f-84832fbe916d",
   "metadata": {},
   "source": [
    "Notice some columns aren't showing, lets see what they are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab8f559f-9d56-4e40-a5dd-af93841b8486",
   "metadata": {},
   "outputs": [],
   "source": [
    "view_middle_columns = structure_ddf.columns[:20]\n",
    "structure_ddf[view_middle_columns].head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65bcb50c-1888-430e-ad88-4fc015310e85",
   "metadata": {},
   "source": [
    "We can also further investigate the impacts of blocksize:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "886f611d-9d3f-4f7e-999f-0c6527fbc4fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "partition40 = structure_ddf.partitions[40].compute()\n",
    "partition40.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8de1406-840b-4d26-b165-f5551e028edd",
   "metadata": {},
   "source": [
    "partition40.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61eaf317-5fe0-4e8d-8c1d-03a0840d2e8e",
   "metadata": {},
   "source": [
    "Each partition has 16mb of data, roughly 873,670 rows. This will vary based on the blocksize chosen when calling the ut.get_US_structures_all() function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ea9c6a1-4f2e-40e4-87e9-2b4697fecdbe",
   "metadata": {},
   "source": [
    "## Census Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9adff1ae-c575-4b33-81bd-de20b0f9b992",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee78f5ff-8424-4427-aa37-18f39c0ffaa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# THIS CAN BE NEXT-STEPS // FURTHER FUTURE ANALYSIS. It does not need to be completed 100%\n",
    "Review Census data within Worcester. Grab the shapefiles for two of the census blocks. spatial filter & intersection with the dataset again.\n",
    "\n",
    "Then review and draw relationships from the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11440c8c-8557-4bf7-b5c9-38c9f6c208e0",
   "metadata": {},
   "source": [
    "##### Also, create a function to download country wide census data (depending on the size. Maybe just do cities/Massachusetts and just provide a link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1616a30b-a507-47e9-8970-7ef2e2a47941",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Understanding it (comparing GeoId)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04ce7de7-9304-4bb8-86e6-6411e3d44185",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "575b0668-1e7e-46d5-8611-7d5111e3f39d",
   "metadata": {},
   "source": [
    "## Data Checks (for spatial joins)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d568f7f6-2073-4bfc-bff8-eaf853b04f6d",
   "metadata": {},
   "source": [
    "### If you go on to review another state. You may need to re-project the CRS for the entire dataset. The following is a good site for visually seeing what the EPSG covers\n",
    "https://spatialreference.org/ref/epsg/2249/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af156ba8-997e-44be-8635-c20d3fef7ca1",
   "metadata": {},
   "source": [
    "##### Coordinate Reference System (CRS)\n",
    "Since this dataset captures \"every structure larger than 450 square feet\" we have a base minimum, however, to get specific building sizes, or plot, we will need to make sure the CRS is for the specific region/area we are analyzing. We will re-project based on the 'geometries' column.\n",
    "\n",
    "First, we will check to see if we need to project to a CRS for the first time, or re-project the current one. To check if the data has a CRS, the following code will suffice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee760479-bf0d-406b-a475-30f54c24d581",
   "metadata": {},
   "outputs": [],
   "source": [
    "structure_ddf.crs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a26c3411-dd89-4308-b93d-2fe937917a75",
   "metadata": {},
   "source": [
    "We will have to re-project for an appropraite EPSG for our area of interest. The entire dataset is in: 4326. \n",
    "\n",
    "Since we are analyzing Massachusetts (City: Worcester, Town: Norwood), and the shapefile we retreived from the City of Worcester was in 2249, we will use 2249.\n",
    "\n",
    "EPSG:2249 covers both Massachusett counties we analyze (Worcester and Norfolk), this is in US Survey feet which means we don't need to worry about converting from meters to feet. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "292338d6-de72-4e87-95d0-9375280db786",
   "metadata": {},
   "outputs": [],
   "source": [
    "structure_ddf.to_crs(epsg=2249)\n",
    "# NOTE: if you analyze another area that this crs is not good for, you will want to make adjustments. For simplicity, we change the crs for the \n",
    "# entire dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4a9903d-4ee9-4eee-a324-bdf6b1b79545",
   "metadata": {},
   "source": [
    "Calculate the square footage for each building and make this stored in a new column 'structure_size':"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c53a4c7a-172b-4754-a37c-98a27beef76c",
   "metadata": {},
   "outputs": [],
   "source": [
    "structure_ddf['structure_size'] = structure_ddf.geometry.area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a3c1fd1b-bf07-443b-b860-c344d87f052d",
   "metadata": {},
   "outputs": [],
   "source": [
    "### geometry column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88729d44-7cf3-4102-9392-7d55d2d32ae3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "25a0699e-10c4-487c-b37a-a1686e8fec64",
   "metadata": {},
   "source": [
    "## Dataset Integrity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b387d20-8a11-4994-88dc-3fb9de4978f3",
   "metadata": {},
   "source": [
    "This section shows an example of questions you need to be asking each time you load in a dataset. It is important to verify work so you know the best way to analyze the dataset\n",
    "\n",
    "\n",
    "Our dataset has columns that specify the state and/or city that a structure is in. However, these columns have blanks as not every structure is identified. For this reason, I want to investigate how many buildings don't get identified.\n",
    "\n",
    "I will use the city of worcester shapefile from the city government's website. Whenever downloading a shapefile, make sure you get all the other files that come with it:\n",
    ".shp, .cpg, .dbf, .prj, .shx, and .xml\n",
    "\n",
    "I'll compare the shapefile to 2 different dataframes:\n",
    "\n",
    "1. The first dataframe will make a dataframe of buildings/rows that are identified as Massachusetts. Then from that dataframe, I will grab a new one for buildings/rows that are identified as Worcester.\n",
    "\n",
    "2. The second dataframe will be the entire dataset, no filtering based on columns.\n",
    "\n",
    "Then I will compute the number of rows for each spatial join with the Worcester Shapefile. Then a user can decide if one is 'better' than the other. You also have to consider computational power. If you only lose 100 out of 100,000 buildings, is it worth it to save computational power? I think yes, but it all depends on your analysis and abilities. \n",
    "\n",
    "You will not see the outputs from most '.compute()' lines of code, that is due to a weak computational power on my personal machine. Any computation should be ran with caution, especially those on the entire dataset. I will try to include warnings before each compute to remind the user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bab267be-9ce5-461e-8e57-520c03bb67e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare 2 below with above 1. Take best of both!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16a865af-87c7-49aa-8e1a-2fe8afec7de5",
   "metadata": {},
   "outputs": [],
   "source": [
    "OVERVIEW:\n",
    "- This is testing the usefulness of the datasets columns for specifying if a structure is in a specific state/city.\n",
    "\n",
    "\n",
    "\n",
    "### If you go on to review another state. You may need to re-project the CRS for the entire dataset. The following is a good site for visually seeing what the EPSG covers\n",
    "https://spatialreference.org/ref/epsg/2249/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff98d5b6-6419-4076-a332-a04d20ef82ec",
   "metadata": {},
   "source": [
    "### Checking the validity of the 'PROP_CITY' column\n",
    "\n",
    "##### Compare the number of rows when using 'PROP_CITY' == 'Worcester' VS a shapefile of Worcester as a Spatial Filter and finding intersections\n",
    "(Using massachusetts_ddf ensures not grabbing Worcester or Boston city data for other states)\n",
    "\n",
    "We will use .shape to compare the number of rows\n",
    "\n",
    "To ensure we use the correct boundaries for Worcester, we will download a shapefile of the layout. This will be downloaded to the 'data' folder and not done in an automatic coding format. However, if you find a different shapefile/city, the same approach can be used. Just mount a folder when you run the docker container from a directory that has the associated city boundary shape files, then you can change the 'dg_read_file' filepath. (Future Implementation: Use selenium to download this file within docker so that users don't need to have data downloaded with the Docker Image)\n",
    "\n",
    "https://opendata.worcesterma.gov/datasets/worcesterma::city-boundary-1/about"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0704e21a-9199-43d6-a3d3-e537b35e54ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "massachusetts_ddf = structure_ddf[structure_ddf['PROP_ST'] == 'Massachusetts']\n",
    "worcester_PROP_CITY_ddf = massachusetts_ddf[massachusetts_ddf['PROP_CITY'] == 'Worcester']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2386fab-5ef2-4090-bcb0-02f30304de8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "worcester_boundary = dg.read_file(f\"./data/Worcester/City_Boundary.shp\", chunksize = 75000) # chunksize specifies number of rows per chunk "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "352bff0b-73ea-4608-95f4-4d3557c18bf5",
   "metadata": {},
   "source": [
    "##### If the above line of code gave you an error similar to:\n",
    "\n",
    "ERROR 1: PROJ: proj_create_from_database: Open of /opt/conda/envs/us_structures_analysis/share/proj failed\n",
    "\n",
    "Try re-running it\n",
    "\n",
    "or\n",
    "\n",
    "Copy the folder that contains all of the shapefile files outside the mounted folder, so it is in the main directory.\n",
    "\n",
    "For instance. I use a mounted 'saved' folder, with a 'data' folder inside that contains all the geoparquets plus a 'Worcester' folder which contains all the files for the Worcester shapefile. I copied the 'Worcester' folder so that it is outside of the 'saved' folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2db3359-2b56-48b9-baad-5b69056247b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# confirm crs\n",
    "worcester_boundary.crs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "326a58ea-d91b-4aa5-9cba-de88a78226be",
   "metadata": {},
   "outputs": [],
   "source": [
    "worcester_boundary.compute() # insight to the shapefile layout."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1968d08-82a2-47d8-a1d0-7cb44121196f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#AS NOTED IN THE DATA CHECKS PORTION, we must check the geometry files are proper for the join.\n",
    "# the structure_ddf.head(5) line and the above worcester_boundary.compute() will show how the geometry columns are named. The prints confirm they are\n",
    "# the same.\n",
    "\n",
    "structure_ddf = structure_ddf.set_geometry('geometry')\n",
    "worcester_boundary = worcester_boundary.set_geometry('geometry')\n",
    "print(structure_ddf.geometry.name)\n",
    "print(worcester_boundary.geometry.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4e5f14a-6798-4e1c-872b-4e04be075f76",
   "metadata": {},
   "source": [
    "##### Computing Disclaimer\n",
    "Here we will count the number of rows that each spatial join gives. The .compute() statements will provide the answers. Run at your own risk (if dask workers 'die' you will likely have to shutdown the docker container and open a new one)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51706f65-2d52-4aef-a3e6-37fe2b33d1b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "worcester_entire_ddf = dg.sjoin(structure_ddf, worcester_boundary, how='inner', predicate='within') # within makes sure the whole structure_ddf \n",
    "                                                                                                       # row is inside the worcester_boundary\n",
    "\n",
    "# Understand this approach may mean some of the structure_ddf rows near the boundary won't be included if the shapefile is off by just a little.\n",
    "\n",
    "worcester_PROP_ddf = dg.sjoin(worcester_PROP_CITY_ddf, worcester_boundary, how='inner', predicate='within')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4a252c9-e270-4076-b0c0-09a958edc37f",
   "metadata": {},
   "outputs": [],
   "source": [
    "entire_df_worcester_rows = worcester_entire_ddf.count().compute()\n",
    "entire_df_worcester_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03c1fd49-86c6-4e3c-b901-5f04f888b835",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_by_columns_worcester_rows = worcester_PROP_ddf.count().compute()\n",
    "filtered_by_columns_worcester_rows"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6bc0bac-3767-426d-8040-d30b3dc1aaff",
   "metadata": {},
   "source": [
    "## Town Example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52907f3d-f9e6-4340-891b-43eda15274cf",
   "metadata": {},
   "source": [
    "The goal of adding a town example is that it will have less computation requirements than a city. I also show some potential plots that can be done. Note: my computer still could not handle this! Also, this is relying on filtering dataset by column names, but the zip codes seemed more filled in (PROP_ZIP) than state or city.\n",
    "\n",
    "User could also grab a shapefile for Norwood and use the original structure_ddf to spatial join with it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2d68592-c544-42f8-8597-f8183f424503",
   "metadata": {},
   "outputs": [],
   "source": [
    "norwood_ddf = massachusetts_ddf[massachusetts_ddf['PROP_ZIP'] == '02062']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b35c58b-620c-4c19-ba02-44653ee4969a",
   "metadata": {},
   "source": [
    "##### Find number of structures in Norwood that are part of dataset (Computing Disclaimer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "173fe63b-0d46-4e7e-b690-ac291930dc37",
   "metadata": {},
   "outputs": [],
   "source": [
    "norwood_ddf.shape[0].compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7e010e2-6119-43df-8b15-9a98ced23d7a",
   "metadata": {},
   "source": [
    "##### Find the average structure size (Computing Disclaimer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c275d9a-52a1-48f1-b133-2daa44f0afb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "norwood_ddf['structure_size'].mean().compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1b9fc0e-7b49-45de-b089-eacc9e45e30e",
   "metadata": {},
   "source": [
    "##### Make a histogram of structure sizes (Computing Disclaimer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d12e797-7a0f-491b-a9f3-e22dae5816fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "norwood_structure_sorted = norwood_ddf.sort_values(by='structure_size', ascending=False).compute()\n",
    "\n",
    "# binLength = len(bldg_norwood_sorted) // 25\n",
    "plt.hist(norwood_structure_sorted['structure_size'], bins = 300, edgecolor='black')\n",
    "plt.xlabel('Area (in ft^2)')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('area_in_square_feet for Norwood Buildings')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43b024ea-6201-4272-a700-559a56fae1fd",
   "metadata": {},
   "source": [
    "## Future Implementation\n",
    "\n",
    "- Utilize selenium python package and create a function that can go onto Worcester website and download the shapefile\n",
    "- Provide plotting examples (was hesitant to attempt since I can't compute anything)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb9f2698-6a60-4a2e-9d66-ce229b3935dc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6ebd826e-ee04-4d69-9d53-944590bfd4c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import dask_geopandas as dg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d94024a-bada-483c-a102-1c89b327d699",
   "metadata": {},
   "source": [
    "# ATTENTION:\n",
    "User must follow below instructions and replace the AWS access key and secret access key. This is crucial for the analysis to work."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7332e0fe-0dc2-4495-90f3-1bc172006b40",
   "metadata": {},
   "source": [
    "##### Utilizing data from: https://source.coop/repositories/wherobots/usa-structures/description"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7b0bb02-eecc-47c2-9759-583f67c57341",
   "metadata": {},
   "source": [
    "All data on Source Cooperative, are hosted on AWS S3 bucket. In order to access them, you need credentials that you can generate on Source Cooperative website. Atfer logging in, click on your name at the top right corner, and then click on your username. Then navigate to \"Manage\" page on the left side. At the bottom of this page you will find a section called \"API Keys\". If no key has been generated before, generate a new one and then copy the values for each of the following keys, and paste them in the following cell.\n",
    "\n",
    "source.coop website: https://source.coop/\n",
    "\n",
    "###### Source: https://github.com/github.com/HamedAlemo/vector-data-tutorial/scalable_vector_analysis.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3c8836e8-2b18-4e9e-b27b-de29fde29f67",
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################\n",
    "#   Read Above 'ATTENTION' Note  #\n",
    "##################################\n",
    "\n",
    "AWS_ACCESS_KEY_ID = \"<YOUR ACCESS KEY>\"\n",
    "AWS_SECRET_ACCESS_KEY = \"<YOUR SECRET ACCESS KEY>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a4ff4cad-0828-4441-ae6c-a97e642e141a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "s3_client = boto3.client('s3',\n",
    "                         aws_access_key_id = AWS_ACCESS_KEY_ID, \n",
    "                         aws_secret_access_key = AWS_SECRET_ACCESS_KEY,\n",
    "                         endpoint_url='https://data.source.coop'\n",
    "                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "299f6bda-b00f-4ef0-8e4c-4477a3ccad88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://127.0.0.1:8787/status\n"
     ]
    }
   ],
   "source": [
    "from dask.distributed import Client, LocalCluster\n",
    "cluster = LocalCluster()\n",
    "client = Client(cluster)\n",
    "print(client.dashboard_link)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "561e1f82-dd29-4a21-acff-a5033ba562af",
   "metadata": {},
   "source": [
    "Local path for downloading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "892320b2-8e2c-499a-8a46-8d480727f016",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If running from analysis file within the 'saved'//mounted folder and do NOT wish to save the raw data - delete the\n",
    "# first local_path and uncomment (remove #) on the second one\n",
    "\n",
    "local_path = \"./data/\"\n",
    "# local_path =\"/data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "202556a2-ad5c-4c69-b9e0-d025708a8b92",
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils as ut"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8be36983-48b5-44e5-9b30-af1efb6268b7",
   "metadata": {},
   "source": [
    "## Understanding the SPECIFIED Edition\n",
    "\n",
    "This approach requires a list of parquet links to be inputted. However, if you desire to only download 1 of the parquet files, you can use the following example as a way to insert only 1 files later on:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2c291d56-2efc-4d78-bf07-4ad98e04db81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['parquet1.parquet']\n",
      "['parquet3.parquet']\n",
      "\n",
      "\n",
      "Once you get to the section for manually inputting the parquet links you want, you can do the same index operations to achieve just 1 parquet.\n"
     ]
    }
   ],
   "source": [
    "example_list_of_parquets = ['parquet1.parquet',\n",
    "                    'parquet2.parquet',\n",
    "                    'parquet3.parquet',\n",
    "                    'parquet4.parquet',\n",
    "                    'parquet5.parquet']\n",
    "\n",
    "print(example_list_of_parquets[0:1]) # returns the first index of the list (0)    Indices start at 0 in Python. Hence the '(0)'.\n",
    "\n",
    "print(example_list_of_parquets[2:3]) # returns the third index of the list (2)\n",
    "\n",
    "print('\\n\\nOnce you get to the section for manually inputting the parquet links you want, you can do the same index operations to achieve just 1 parquet.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f2b9d38-cfd9-472f-aea8-93b10872ea9b",
   "metadata": {},
   "source": [
    "### Walkthrough\n",
    "\n",
    "I will walk you through an example of using the specified code that will automatically grab parquet files with the unique number identifier of 00000 through 00004. NOTE: the 'endfix' variable is for the 12/6/2024 update. It is very likely to be different once you run this package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83383939-7d31-4332-b2b8-b13b768ee93c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If running from analysis file within the 'saved'//mounted folder and do NOT wish to save the raw data to your machine - delete the\n",
    "# first local_path and uncomment (remove #) on the second one\n",
    "\n",
    "local_path = \"./data/\" # saves data to machine\n",
    "# local_path =\"/data/\" # deletes data after removing the container"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27a19c11-c25b-459e-b5a2-aee52c5c654e",
   "metadata": {},
   "source": [
    "As of the 12/6/2024 download, you can go to the following page to see all of the parquet files:\n",
    "\n",
    "https://source.coop/wherobots/usa-structures\n",
    "\n",
    "To the right of the .parquet files will show the size of them.\n",
    "Click on one of the .parquet file links, it should bring you to a window that details the file, for instance:\n",
    "\n",
    "#### File Name\n",
    "\n",
    "part-00000-170892fe-0fe0-43c1-999d-f0911ce43365-c000.zstd.parquet\n",
    "\n",
    "#### File Size\n",
    "\n",
    "2,058,402,104 Bytes (1.9 GiB)\n",
    "\n",
    "#### Content Type\n",
    "\n",
    "application/octet-stream\n",
    "\n",
    "#### Last Modified\n",
    "\n",
    "Fri Dec 06 2024 15:29:54 GMT-0500 (Eastern Standard Time)\n",
    "\n",
    "#### Data URL\n",
    "\n",
    "https://data.source.coop/wherobots/usa-structures/geoparquet/part-00000-170892fe-0fe0-43c1-999d-f0911ce43365-c000.zstd.parquet\n",
    "\n",
    "#### S3 URI\n",
    "\n",
    "s3://wherobots/usa-structures/geoparquet/part-00000-170892fe-0fe0-43c1-999d-f0911ce43365-c000.zstd.parquet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29efb19b-3e40-47ee-a4a5-d7fd108aa0d1",
   "metadata": {},
   "source": [
    "# We are using an S3 client approach.\n",
    "### Therefore, focus on the S3 URI, this is what we use to download the data\n",
    "\n",
    "The bucket_name is 'whereobots'\n",
    "\n",
    "The prefix is everything after the bucket_name AND it's slash (/),  and everything before the unique #-identifier, 00000.\n",
    "\n",
    "The endfix is everything after the unique #-identifier, including the hyphen.\n",
    "\n",
    "#### Copy and paste any adjustments to the file link that have occurred since this update. It is very likely the bucket_name and prefix will be the same.\n",
    "\n",
    "#### In the code cell below, if you wish to automatically grab 'x' number of files, change the 'range(0, 5)' portion. This goes in numeric order:\n",
    "\n",
    "00000, 00001, 00002, 00003, 00004\n",
    "\n",
    "#### You can adjust the starting number to be something besides 0, and whichever number you want to end on, remember it is NOT inclusive. To go from 0-4, you must use 5 as the end number for the range function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec1c158a-46df-4661-af92-2a2614214cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket_name = 'wherobots'  # (Bucket name is the account the posted the dataset)?\n",
    "prefix = \"usa-structures/geoparquet/part-\"\n",
    "\n",
    "endfix = \"-170892fe-0fe0-43c1-999d-f0911ce43365-c000.zstd.parquet\"\n",
    "\n",
    "links = [f\"{prefix}{str(i).zfill(5)}{endfix}\" for i in range(0, 5)] # originally 0-200, datset updated mid project, now goes to 00009\n",
    "links"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9a3250e-cc7d-4742-911c-6d399914c9e5",
   "metadata": {},
   "source": [
    "Going back to the beginning, you can specify if you'd prefer only 1 of the links:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08dafeff-ad2c-4119-bf8e-18882cb774d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "link_list = links[0:1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f1f37cc-dc95-4c98-84ef-bee0be13ddf3",
   "metadata": {},
   "source": [
    "Or use the entire list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdb1d4ac-0ad0-41da-b3b5-2c754e974dd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "link_list = links"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fe60a44-68a6-476b-911a-097738537073",
   "metadata": {},
   "source": [
    "Make sure to only run the cell block you desire. If you aren't sure, we can print the 'link_list' variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2b5c96d-9649-4127-a91f-4270ee0e31df",
   "metadata": {},
   "outputs": [],
   "source": [
    "link_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3de32cb1-9539-41d9-bdca-d39124e90bdf",
   "metadata": {},
   "source": [
    "Finally, we will create the dask dataframe with your desired links. Now you may start analysis. Use code from 'basic_analysis_ALL' as an example, but remember the warning from the README.md file. If you are trying to do a specific location, you will likely need to test several parquet files before you find it. AND there could be a situation where 2 or more parquet files cover your desired area. Proceed with caution as this could be the start of a LONG testing process. Good luck!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd1ec936-ca68-4a2b-9beb-6ca04d76ad6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "structure_ddf = ut.get_US_structures_specified(bucket_name, link_list, s3_client, local_path, blocksize = \"16M\") #256M is regular block size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "531b9a92-ff19-454b-9f67-109878c77d2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "structure_ddf.head(5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
